Here's a draft Twitter thread for GRIT:
ğŸš€ Excited to share GRIT: Teaching MLLMs to Think with Images! Our new approach enables models to generate reasoning chains that interleave natural language with bounding box coordinates ğŸ“ŠğŸ§  #GRIT #MLLM #VisualReasoning ğŸ§µ1/5
ğŸ“ Paper: [paper_link]
ğŸ’» Code: [code_link]
ğŸ§µ2/5 GRIT introduces a Grounded Reasoning Paradigm where models explicitly reference visual regions during reasoning, creating a clear link between language and what they're looking at in the image!
No more vague "I can see something in the corner" - now models say exactly WHERE they're looking ğŸ‘€
ğŸ§µ3/5 We developed GRPO-GR, a reinforcement learning algorithm with smart rewards that teach models to ground their reasoning in images super efficiently.
The best part? It only needs 20 training examples! No manual annotations or bounding box labels required ğŸ”¥
ğŸ§µ4/5 GRIT-trained models outperform baselines across diverse visual reasoning tasks by unifying two abilities that were previously separate in MLLMs:
Answer accuracy (measured by GPT-4o judge)
Visual grounding precision (measured by IoU with ground truth)
ğŸ§µ5/5 Check out what GRIT can do:
Count objects with precise region identification
Understand spatial relationships by grounding objects
Correctly reason about missing objects without hallucinating
This is a big step toward AI that can truly think with images! ğŸ“¸ğŸ¤–
This work was made possible by our amazing team at UC Santa Cruz and eBay! Thanks to @[Your Twitter] @[Coauthor Twitters]
For all the details, check out our paper and try our code! ğŸ™